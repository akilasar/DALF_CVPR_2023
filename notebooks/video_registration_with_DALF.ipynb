{"cells":[{"cell_type":"markdown","metadata":{"id":"XnBrT_c10di_"},"source":["## Clone DALF repo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqCM5qBfz-7I"},"outputs":[],"source":["!git clone 'https://github.com/verlab/DALF_CVPR_2023.git'\n","\n","WORKING_DIR = '/content/DALF_CVPR_2023'\n","import sys\n","sys.path.append(WORKING_DIR)"]},{"cell_type":"markdown","metadata":{"id":"Ythr9Qjg0jnv"},"source":["# Initialize DALF model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N_mGOXI086vP"},"outputs":[],"source":["from modules.models.DALF import DALF_extractor as DALF\n","import torch\n","import cv2\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","dalf = DALF(dev = device)"]},{"cell_type":"markdown","metadata":{"id":"zqX09gKvbGOf"},"source":["# Video registration with DALF\n","Here, we open a .mp4 video, use the first frame as the template, and track the template along the video. Please use GPU for faster inference, otherwise it may take a while."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7yzlnD0bFLH"},"outputs":[],"source":["from modules.tps import RANSAC\n","from modules.tps import numpy as tps_np\n","from modules.tps import pytorch as tps_pth\n","\n","import torch.nn.functional as F\n","\n","import tqdm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def warp_image_cv(img, c_src, c_dst, dshape = None):\n","    img = torch.tensor(img).to(device).permute(2,0,1)[None, ...].float()\n","    dshape = dshape or img.shape\n","    theta = tps_np.tps_theta_from_points(c_src, c_dst, reduced=True, lambd=0.01)\n","    theta = torch.tensor(theta).to(device)[None, ...]\n","    grid = tps_pth.tps_grid(theta, torch.tensor(c_dst, device=device), dshape)\n","    #print(grid.shape, grid.dtype)\n","    img = F.grid_sample(img, grid, align_corners=False)\n","    return img[0].permute(1,2,0).cpu().numpy().astype(np.uint8)\n","\n","# Open the MP4 file\n","cap = cv2.VideoCapture(WORKING_DIR + '/assets/deform_bag.mp4') \n","\n","ret, img1 = cap.read()\n","kps1, descs1 = dalf.detectAndCompute(img1)\n","nframe = 0\n","\n","in_frames = []\n","out_frames = []\n","\n","# Loop through the frames\n","while cap.isOpened():\n","    # Read a frame\n","    ret, img = cap.read()\n","    if ret:\n","        in_frames.append(img)\n","    else:\n","        break\n","    \n","\n","for img2 in tqdm.tqdm(in_frames):\n","    #Compute DALF features\n","    kps2, descs2 = dalf.detectAndCompute(img2)\n","\n","    #Match using vanilla opencv matcher\n","    matcher = cv2.BFMatcher(crossCheck = True)\n","    matches = matcher.match(descs1, descs2)\n","\n","    src_pts = np.float32([kps1[m.queryIdx].pt for m in matches])\n","    tgt_pts = np.float32([kps2[m.trainIdx].pt for m in matches])\n","\n","    #Computes non-rigid RANSAC\n","    inliers = RANSAC.nr_RANSAC(src_pts, tgt_pts, device,  batch = 3_000, thr = 0.2)\n","    good_matches = [matches[i] for i in range(len(matches)) if inliers[i]]\n","\n","    h, w = img1.shape[:2]\n","\n","    c_src = np.float32([kps1[m.queryIdx].pt for m in good_matches]) / np.float32([w,h])\n","    c_dst = np.float32([kps2[m.trainIdx].pt for m in good_matches]) / np.float32([w,h])\n","\n","    #Warp deformed image (img2) into template\n","    warped = warp_image_cv(img2, c_dst, c_src)\n","\n","    result = np.hstack([cv2.resize(img1, (w // 4, h // 4)),\n","                        cv2.resize(img2, (w // 4, h // 4)),\n","                        cv2.resize(warped, (w // 4, h // 4))])\n","    out_frames.append(result)\n","\n","    #plt.imshow(result), plt.show()\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x3fIqlNJ0EAI"},"source":["#Generate GIF\n","Finally, we generate a GIF with the tracking result and save it! This gif is used in the README of DALF's git repo ðŸ”¥"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XtiU1XIitSpW"},"outputs":[],"source":["#Generate GIF\n","\n","import cv2\n","import imageio\n","import numpy as np\n","\n","with imageio.get_writer(\"out.gif\", mode=\"I\") as writer:\n","    # Loop through the frames and write them to the GIF writer object\n","    for frame in tqdm.tqdm(out_frames):\n","        writer.append_data(frame[..., ::-1])\n","\n","from IPython.display import Image\n","Image(open('out.gif','rb').read())"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1kD0aJ_v6sdtvLtW5H-xMERWrPGPwPMMY","timestamp":1682631763798}],"authorship_tag":"ABX9TyNasalGUQ/6d/pQRyryhi7a"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}